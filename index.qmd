---
title: "Random Forest Algorithm"
author: "Amy Brown, Jeff Eddy, Armani Harris"
date: "July 23, 2023"
format:
  html:
    code-fold: true
course: STA 6257 - Advance Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
---

## Introduction

Random Forests (RF) is a popular ensemble learning algorithm that combines multiple decision trees to improve predictive accuracy and handle complex datasets. This literature review aims to explore ten articles that discuss various aspects of the Random Forest algorithm, including its theoretical foundation, feature selection, data imbalance resolution, interpretability, and comparisons with other classification methods. The selected articles offer valuable insights into the strengths, weaknesses, and practical applications of Random Forest in different domains.

### Literature Review

Breiman [@Breiman2001] introduced the concept of Random Forests and highlighted the importance of the Law of Large Numbers in avoiding overfitting. Liaw and Wiener [@Liaw2002] expanded on this work, focusing on the classification and regression capabilities of RF. They discussed the use of bagging and highlighted the significance of variable importance in RF models.When dealing with datasets containing a high number of variables the importance of feature selection is discussed by Jaiswal, K. & Samikannu, R. [@Jaiswal2017]. Han and Kim investigated the optimal size of candidate feature sets in Random Forest for classification and regression tasks. The study reveals that the default candidate feature set is not always the best performing and that the optimal size can vary depending on the dataset used [@Han2019]. The potential challenges such as overfitting and handling multi-valued attributes exist when using the Random Forest (RF) algorithm. However, the advantages emphasized by Abdulkareem, N. M., & Abdulazeez, A. M entail accuracy, efficiency with large datasets, and minimal data preprocessing requirements [@Abdulkareem2021]. To address the issue of data imbalance, More and Rana [@More2017] reviewed techniques for resolving imbalanced datasets using RF. The various techniques include sampling, weighted Random Forest, and cost-sensitive methods, to handle imbalanced datasets. A practical application of Random Forest in computational biology and bioinformatics was conducted by Boulesteix, A. L. et al. [@Ali2012], providing insights into decision tree construction, handling bias, and recommendations for parameter tuning. Schonlau and Zou [@Schonlau2020] focused on optimizing RF models by tuning parameters and subtree iterations. They discussed the use of out-of-bag (OOB) samples to approximate the model's error during training. Zhu proposes a new algorithm, the SearchSize method, which estimates the optimal feature set size using out-of-bag error methodology [@Tongtian2020]. Tuning these parameters helps achieve higher prediction accuracy Schonlau, M., & Zou, R. Y. [@Schonlau2020]. Aria et al. [@Aria2022] discussed internal approaches that provide a global overview of the model. They mentioned the Mean Decrease Impurity (MDI) as a measure of variable importance and emphasized the visualization of RF classifiers using toolkits.

## Methods

Random Forest algorithm is a supervised learning model consisting of multiple uncorrelated un-pruned decision trees creating a forest. A decision tree starts at the tree root node and splits into internal nodes. The tree continues to grow as the data splits on the internal node.  The tree ends with leaf node when the data splits have stopped. For the Random Forest algorithm, two sampling methods types were reviewed using the r caret package: repeated k-fold cross-validation and bootstrapping [@Abdulkareem2021].

The basic steps of the Random Forest algorithm are as follows:

1. Randomly select k features from a total of m features where k < m.
2. Specify a node size d amoung the k features in step 1 using the best split point discussed in step 3.
3. For bootstrap sampling method, each node is split into additional internal nodes using Gini Impurity. The Gini Impurity is calculated by summing the Gini decrease across every tree in the forest and divided by the number of trees in the forest to provide an average. For the repeated k-fold cross-validation, each node is split into additional internal nodes using Permuation Importance. The Permutation Importance is calculated by evaluating the decrease in accuracy when a feature is removed and further divided into classes.
4. Repeat the previous steps 1 through 4 steps until l node is remaining.
5. Continue to build the forest by repeating steps 1 through 4 for n number of times to create n number of trees [@Jaiswal2017].

The Random Forest algorithm can be used for classification or regression models. The target selected for the algorithm identifies if the algorithm will use a classification or regression model. The leaf node will predict a category if a classification model is identified and a numerical value for a regression model. The prediction is determined by taking the majority vote of all leaf nodes for classification and an average of the leaf nodes for regression. This Random Forest algorithm process is represented in the flowchart of Figure 1 [@Abdulkareem2021].

![Figure 1 [@Abdulkareem2021]](RF%20flowchart.png)

## Analysis and Results

### Packages

```{r, message=FALSE}
library(caret)
library(boot)
library(tidymodels)
library(lindia)
library(car)
library(haven)
library(htmltools)
library(tidyverse)
library(dplyr)
library(fastDummies)
library(nnet)
library(randomForest)
library(gridExtra)
library(AppliedPredictiveModeling)
library("ggplot2")
library("reshape2")
```

### Load Data

```{r, message=FALSE}
data<- read.csv('C:/Users/amybr/OneDrive/Desktop/dataset_heart.csv')
```

### Data and Visualization

The Random Forest algorithm is reviewed using the Heart Disease Prediction dataset. There are 270 observations in the dataset without any missing values. The target is heart disease predicting the presence or absence of heart disease. The target and the other 13 features are listed as follows [@Singh2023]:

<a href="https://www.kaggle.com/datasets/utkarshx27/heart-disease-diagnosis-dataset">Link to Documentation for Heart Disease Prediction data</a>

<b>Heart Disease Prediction Features and Target:</b>

 1. age\
 2. sex\
 3. chest pain type (4 values)\
 4. resting blood pressure\
 5. serum cholestoral in mg/dl\
 6. fasting blood sugar 120 mg/dl\
 7. resting electrocardiographic results (values 0,1,2)\
 8. maximum heart rate achieved\
 9. exercise induced angina\
10. oldpeak = ST depression induced by exercise relative to rest\
11. the slope of the peak exercise ST segment\
12. number of major vessels (0-3) colored by flourosopy\
13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\
14. Target (Absence (1) or presence (2) of heart disease) [@Singh2023]\

To visualize the data, two boxplots were created to scale the features accordingly. An extreme outlier was observed in the feature serum cholesterol. The datapoint will be removed before running the model.

```{r, message=FALSE}
# Split dataframe into dataframe for box plots only
data_gt_100=data
data_lt_100=data

# Boxplot age, serum cholesterol, resting blood pressure, and max heart rate
data_gt_100 = data %>% select(1,4,5,8)

# Reshape sample data to long form
data_gt_100_melt <- melt(data_gt_100)

# Add variable parameter for axis label in dataset
levels(data_gt_100_melt$variable) <- c("aging","resting.blood.pressure","serum.cholesterol","max.heart.rate")
  
# Draw boxplot
ggplot(data_gt_100_melt, aes(variable, value)) + 
geom_boxplot(width = .25)  +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

# Boxplot of all other variables
data_lt_100 = data_lt_100 %>% select(2,3,6,7,9:14)

# Reshape sample data to long form
data_lt_100_melt <- melt(data_lt_100)

# Add variable parameter for axis label in dataset
levels(data_lt_100_melt$variable) <- c("sex","chest.pain.type","fasting.blood.sugar","resting.electrocardiographic.results", "exercise.induced.angina","oldpeak","ST.segment","major.vessels", "thal", "heart.disease")
  
# Draw boxplot
ggplot(data_lt_100_melt, aes(variable, value)) + 
geom_boxplot(width = .25)  +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

```

### Data Transformation

Setting the target variable to factor. The thal feature was recoded for clarity and set as a factor variable.

```{r, message=FALSE}
```

```{r, message=FALSE}
data$heart.disease<- as.factor(data$heart.disease)
data1=data
data1$thal[data1$thal== 3]<-"normal"
data1$thal[data1$thal== 6]<-"fixed defect"
data1$thal[data1$thal== 7]<-"reversible defect"
data1$thal<- as.factor(data1$thal)
```

### Removal of Extreme Outlier

The extreme outlier for the feature serum cholesterol is removed from the dataset and validated in the boxplot.

```{r, message=FALSE}
# Remove extreme outlier
data_gt_100 = data_gt_100[data$serum.cholestoral != "564", ]
data1 = data1[data1$serum.cholestoral != "564", ]


# Rerun boxplot after extreme outlier removed 

# Reshape sample data to long form
data_gt_100_melt <- melt(data_gt_100)

# Add variable parameter for axis label in dataset
levels(data_gt_100_melt$variable) <- c("aging","resting.blood.pressure","serum.cholesterol","max.heart.rate")
  
# Draw boxplot
ggplot(data_gt_100_melt, aes(variable, value)) + 
geom_boxplot(width = .25)  +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```

### Model Training

The target variable was binary with categories of no heart disease (1) and heart disease (2). After removing the outlier observation in cholesterol, 269 observations were used to create a random forest algorithm to predict the presence of heart disease. Using the caret package in R, the data set was split for a training model and a testing model at a 0.7 ratio. This resulted in 189 observations in the training data set and 80 in the testing data set. The training dataset was then assessed for the best parameter options that produced the greatest estimated accuracy. Algorithm parameters are defined as such:

Ntree: The number of trees.

Mtry: Controls the amount of randomness that is added to the process. The number of randomly sampled variables drawn at each split.

Terminal nodes: The number of leaf nodes in the tree.

Max nodes: A way to control the complexity of the trees. Maximum number of terminal nodes trees in the forest can have.

Nodesize: Minimum size of terminal nodes.

Feature: A set of inputs.

The testing dataset was then passed into the tuned algorithm to create a predictive model.

```{r, message=FALSE}
set.seed(123)
split <- createDataPartition(data1$heart.disease, p = 0.7, list = FALSE)
train <- data1[split, ]
test <- data1[-split, ]
```

#### K-Fold Cross-Validation Sampling Method

A default training model using repeated k-fold cross-validation sampling method based on classification was created to establish a baseline accuracy. Random Forests are predictive models that form multiple decision trees. At each tree node, a subset of the 13 predictor features were used to introduce randomness into the model. The default training model used 500 decision trees (ntree=500) and determined the optimal number of features to compare to be 2 (mtry= 2). For this classification RF, the decision at each tree node was conducted by permutation importance by removing the association between the feature and target. Accuracy of the default model was 0.8472.

```{r, message=FALSE}
set.seed(123)
# Define the control
trControl <- trainControl(method = "cv",
    number = 10,
    search = "grid")
#model with default values
rf_default <- train(heart.disease~.,
    data = train,
    method = "rf",
    metric = "Accuracy",
    trControl = trControl)
# Print the results
print(rf_default)
```

The training model was assessed for the best possible number of decision trees, nodes, and number of features to consider at each node. After performing the following procedures, the greatest accuracy was determined to be 200 trees with a node size of 14, max nodes of 8, and 2 features considered at each node. The first tuning parameter evaluated was the number of features the model could consider at each node in a tree.

```{r, message=FALSE}
#Search best mtry
set.seed(123)
tuneGrid <- expand.grid(.mtry = c(1: 10))
rf_mtry <- train(heart.disease~.,
    data = train,
    method = "rf",
    metric = "Accuracy",
    tuneGrid = tuneGrid,
    trControl = trControl,
    importance = TRUE,
    nodesize = 14,
    ntree = 300)
print(rf_mtry)
print(rf_default)

best_mtry <- rf_mtry$bestTune$mtry 
```

The subsequent procedure evaluates the optimal number of max nodes for the model.

```{r, message=FALSE}
# Search maxnodes - look for highest accuracy

store_maxnode <- list()
# Assigns best mtry to tuneGrid
tuneGrid <- expand.grid(.mtry = best_mtry)
for (maxnodes in c(5: 15)) {
    set.seed(123)
    rf_maxnode <- train(heart.disease~.,
        data = train,
        method = "rf",
        metric = "Accuracy",
        tuneGrid = tuneGrid,
        trControl = trControl,
        importance = TRUE,
        nodesize = 14,
        maxnodes = maxnodes,
        ntree = 300)
    current_iteration <- toString(maxnodes)
    store_maxnode[[current_iteration]] <- rf_maxnode
}
results_mtry <- resamples(store_maxnode)
summary(results_mtry)
```

The final parameter evaluation was the ideal number of trees for the model. From the output of the code, investigators observed the accuracy of the model stabilizes at 200 trees.

```{r, message=FALSE}
# Search best ntree
store_maxtrees <- list()
for (ntree in c(100, 200, 300, 400, 500, 600, 700, 800, 900, 1000)) {
    set.seed(123)
    rf_maxtrees <- train(heart.disease~.,
        data = train,
        method = "rf",
        metric = "Accuracy",
        tuneGrid = tuneGrid,
        trControl = trControl,
        importance = TRUE,
        nodesize = 14,
        maxnodes = 8,
        ntree = ntree)
    key <- toString(ntree)
    store_maxtrees[[key]] <- rf_maxtrees
}
results_tree <- resamples(store_maxtrees)
summary(results_tree)
```

The final training model produced an accuracy of 0.8680 compared to the default training model with an accuracy of 0.8472, a 2.08% improvement in accuracy over the default RF parameters.

```{r, message=FALSE}
# Train final model
set.seed(123)
fit_rf <- train(heart.disease~.,
    data = train,
    method = "rf",
    metric = "Accuracy",
    tuneGrid = tuneGrid,
    trControl = trControl,
    importance = TRUE,
    nodesize = 14,
    ntree = 200,
    maxnodes = 8)
print(fit_rf)
```

#### Bootstrap Sampling Method

A default training model using bootstrap sampling method based on classification was created to compare with the repeated k-fold cross-validation sampling method. Bootstrap does not need cross-validation. Random Forests are predictive models that form multiple decision trees. At each tree node, a subset of the 13 predictor features were used to introduce randomness into the model. The default training model used 500 decision trees (ntree=500) and determined the optimal number of features to compare to be 2 (mtry= 2). For this classification RF, the decision at each tree node was conducted by Ginni Impurity, which is a process of estimating the likelihood that a feature may be misclassified at random, then the feature with the lowest likelihood of misclassification is selected at that node. Accuracy of the default model was 0.8330 compared to .8472 of the repeated -fold cross-validation sampling method.

```{r, message=FALSE}
set.seed(123)
fit_rf2 <- train(heart.disease~.,
    data = train,
    method = "rf",
    importance = TRUE)
print(fit_rf2)
```

To compare the final training settings in the repeated k-fold cross-validation sampling method to the bootstrap sampling method, the same settings were used to train. The bootstrap sampling method is still slightly lower than the repeated k-fold cross-validation Sampling Method with .8367 accuracy compared to .8680. Since repeated k-fold cross-validation sampling method is higher in Accuracy, the predictive modeling will be based off from the repeated k-fold cross-validation sampling method.

```{r, message=FALSE}
# Train final model
set.seed(123)
fit_rf2 <- train(heart.disease~.,
    data = train,
    method = "rf",
    tuneGrid = tuneGrid,
    importance = TRUE,
    nodesize = 14,
    ntree = 200,
    maxnodes = 8)
print(fit_rf2)
```

## Predictive Modeling

```{r, message=FALSE}
# Evaluate model
prediction <-predict(fit_rf, test)
```

Results from the predictive test model indicated that the feature major.vessels was the most influential feature when predicting heart disease from the current dataset with a variable importance score of 100.00. The five most influential features all had variable importance scores above 60.00. These features were major vessels, thal normal, old peak, chest pain type, max heart rate, and thal reversible defect.

```{r, message=FALSE}
varImp(fit_rf)
rfImp<- varImp(fit_rf)
ggplot(rfImp, aes(x = importance(), y = value)) +
  geom_bar(stat = "identity", linewidth = 0.5,fill="blue")
```

Accuracy for the predictive model was calculated to be 82.5% (0.825, CI: 0.7238, 0.9009, p\< 0.001). Sensitivity and specificity were calculated at 0.909 and 0.722 respectively.

```{r, message=FALSE}
cm <- confusionMatrix(prediction, test$heart.disease)

draw_confusion_matrix <- function(cm) {
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # Create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'Positive', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Negative', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'True', cex=1.2, srt=90)
  text(140, 335, 'False', cex=1.2, srt=90)

  # Add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # Add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # Add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  

draw_confusion_matrix(cm)
```

The model has an estimated false positive rate of 28.57%. False positive rate is calculated by the number of false positives divided by the sum of false positives and true negatives. FPR= FP/(FP + TN).

```{r, message=FALSE}
4/(4+10)
```

### Conclusion

In closing, the Random Forest algorithm is a powerful supervised learning model that combines multiple uncorrelated decision trees to form a forest. Two sampling methods were reviewed: repeated k-fold cross-validation and bootstrap. In repeated k-fold cross-validation, decision trees are constructed based on Permutation Importance, while in bootstrap training data is Gini Impurity. The algorithm can be applied to both classification and regression tasks, with the prediction determined by the majority vote or average of the leaf nodes, respectively. The Random Forest algorithm offers a versatile approach to solving complex problems and is widely used in various domains of machine learning and data analysis.

## References
