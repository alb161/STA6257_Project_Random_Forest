<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Amy Brown, Jeff Eddy, Armani Harris">
<meta name="dcterms.date" content="2023-07-12">

<title>Random Forest Algorithm</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Random Forest Algorithm</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Amy Brown, Jeff Eddy, Armani Harris </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 12, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Random Forests (RF) is a popular ensemble learning algorithm that combines multiple decision trees to improve predictive accuracy and handle complex datasets. This literature review aims to explore ten articles that discuss various aspects of the Random Forest algorithm, including its theoretical foundation, feature selection, data imbalance resolution, interpretability, and comparisons with other classification methods. The selected articles offer valuable insights into the strengths, weaknesses, and practical applications of Random Forest in different domains.</p>
<section id="literature-review" class="level3">
<h3 class="anchored" data-anchor-id="literature-review">Literature Review</h3>
<p>Breiman <span class="citation" data-cites="Breiman2001">(<a href="#ref-Breiman2001" role="doc-biblioref">Breiman 2001</a>)</span> introduced the concept of Random Forests and highlighted the importance of the Law of Large Numbers in avoiding overfitting. Liaw and Wiener <span class="citation" data-cites="Liaw2002">(<a href="#ref-Liaw2002" role="doc-biblioref">Liaw and Wiener 2002</a>)</span> expanded on this work, focusing on the classification and regression capabilities of RF. They discussed the use of bagging and highlighted the significance of variable importance in RF models.When dealing with datasets containing a high number of variables the importance of feature selection is discussed by Jaiswal, K. &amp; Samikannu, R. <span class="citation" data-cites="Jaiswal2017">(<a href="#ref-Jaiswal2017" role="doc-biblioref">Jaiswal and Samikannu 2017</a>)</span>. Han and Kim investigated the optimal size of candidate feature sets in Random Forest for classification and regression tasks. The study reveals that the default candidate feature set is not always the best performing and that the optimal size can vary depending on the dataset used <span class="citation" data-cites="Han2019">(<a href="#ref-Han2019" role="doc-biblioref">Han and Kim 2019</a>)</span>. The potential challenges such as overfitting and handling multi-valued attributes exist when using the Random Forest (RF) algorithm. However, the advantages emphasized by Abdulkareem, N. M., &amp; Abdulazeez, A. M entail accuracy, efficiency with large datasets, and minimal data preprocessing requirements <span class="citation" data-cites="Abdulkareem2021">(<a href="#ref-Abdulkareem2021" role="doc-biblioref">Abdulkareem and Abdulazeez 2021</a>)</span>. To address the issue of data imbalance, More and Rana <span class="citation" data-cites="More2017">(<a href="#ref-More2017" role="doc-biblioref">More and Rana 2017</a>)</span> reviewed techniques for resolving imbalanced datasets using RF. The various techniques include sampling, weighted Random Forest, and cost-sensitive methods, to handle imbalanced datasets. A practical application of Random Forest in computational biology and bioinformatics was conducted by Boulesteix, A. L. et al. <span class="citation" data-cites="Ali2012">(<a href="#ref-Ali2012" role="doc-biblioref">Ali et al. 2012</a>)</span>, providing insights into decision tree construction, handling bias, and recommendations for parameter tuning. Schonlau and Zou <span class="citation" data-cites="Schonlau2020">(<a href="#ref-Schonlau2020" role="doc-biblioref">Schonlau and Yuyan Zou 2020</a>)</span> focused on optimizing RF models by tuning parameters and subtree iterations. They discussed the use of out-of-bag (OOB) samples to approximate the model’s error during training. Zhu proposes a new algorithm, the SearchSize method, which estimates the optimal feature set size using out-of-bag error methodology <span class="citation" data-cites="Tongtian2020">(<a href="#ref-Tongtian2020" role="doc-biblioref">Zhu 2020</a>)</span>. Tuning these parameters helps achieve higher prediction accuracy Schonlau, M., &amp; Zou, R. Y. <span class="citation" data-cites="Schonlau2020">(<a href="#ref-Schonlau2020" role="doc-biblioref">Schonlau and Yuyan Zou 2020</a>)</span>. Aria et al. <span class="citation" data-cites="Aria2022">(<a href="#ref-Aria2022" role="doc-biblioref">Aria 2022</a>)</span> discussed internal approaches that provide a global overview of the model. They mentioned the Mean Decrease Impurity (MDI) as a measure of variable importance and emphasized the visualization of RF classifiers using toolkits.</p>
</section>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<p>Random Forest algorithm is a supervised learning model consisting of multiple uncorrelated un-pruned decision trees creating a forest. A decision tree starts are the root node and splits into internal nodes based on the features divided into classes. Further internal nodes are created based on the data split on the internal node. The decision tree ends at the leaf node when all the data belongs to the same class <span class="citation" data-cites="Ali2012">(<a href="#ref-Ali2012" role="doc-biblioref">Ali et al. 2012</a>)</span>. The Random Forest algorithm can be used for classification or regression models. The target selected for the algorithm identifies if the algorithm will use a classification or regression model. The leaf node will predict a category if a classification model is identified and a numerical value for a regression model. The prediction is determined by taking the majority vote of all leaf nodes for classification and an average of the leaf nodes for regression. This process is represented in the flowchart of Figure 1 <span class="citation" data-cites="Abdulkareem2021">(<a href="#ref-Abdulkareem2021" role="doc-biblioref">Abdulkareem and Abdulazeez 2021</a>)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="RF%20flowchart.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure 1 <span class="citation" data-cites="Abdulkareem2021">(<a href="#ref-Abdulkareem2021" role="doc-biblioref">Abdulkareem and Abdulazeez 2021</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="analysis-and-results" class="level2">
<h2 class="anchored" data-anchor-id="analysis-and-results">Analysis and Results</h2>
<p>The target variable was binary with categories of no heart disease (1) and heart disease (2). After removing the outlier observation in cholesterol, 269 observations were used to create a random forest algorithm to predict the presence of heart disease. Using the caret package in R, the dataset was split for a training model and a testing model at a 0.7 ratio. This resulted in 189 observations in the training dataset and 80 in the testing dataset.</p>
<p>A default training model based on classification was then created to establish a baseline accuracy. Random Forests are predictive models that form multiple decision trees. At each tree node, a subset of the 13 predictor features were used to introduce randomness into the model. The default training model used 500 decision trees (ntree=500) and determined the optimal number of features to compare to be 2 (mtry= 2). For this classification RF, the decision at each tree node was conducted by Ginni impurity, which is a process of estimating the likelihood that a feature may be misclassified at random, then the feature with the lowest likelihood of misclassification is selected at that node.</p>
<p>The training model was assessed for the best possible number of decision trees, nodes, and number of features to consider at each node. The greatest accuracy was determined to be 200 trees with a node size of 8 and 2 features considered at each node. The final training model produced an accuracy of 0.8947 compared to the default training model with an accuracy of 0.8472, a 4.75% improvement in accuracy over the default RF parameters.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>Results from the predictive test model indicated that the feature major.vessels was the most influential feature when predicting heart disease from the current dataset with a variable importance score of 100.00. The five most influential features all had variable importance scores above 80.00. These features were blood vessels, chest pain type, thal reversible defect, normal thal, and max heart rate. All five features had a mean decrease Ginni score above 2.2. Resting blood pressure had the lowest variable importance with a score of 0.00 indicating that the feature did not significantly improve the model’s performance in predicting heart disease within this dataset. Fasting blood sugar was the far less significant than the other features as well, with a score of 4.03. Accuracy for the predictive model was calculated to be 82.5% (0.825, CI: 0.7238, 0.9009, p&lt; 0.001). Sensitivity and specificity were calculated at 0.886 and 0.75 respectively. The model has an estimated false positive rate of 25%.</p>
<section id="data-and-visualization" class="level3">
<h3 class="anchored" data-anchor-id="data-and-visualization">Data and Visualization</h3>
<p>The Random Forest algorithm is reviewed using the Heart Disease Prediction dataset. There are 270 observations in the dataset without any missing values. The target is heart disease predicting the presence or absence of heart disease. The target and the other 13 features are listed in Figure 2 <span class="citation" data-cites="Singh2023">(<a href="#ref-Singh2023" role="doc-biblioref">Singh 2023</a>)</span>. The boxplot of each feature is plotted in Figures 3. There is one extreme outlier in feature serum cholesterol. The datapoint will be removed before running the model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Features%20and%20Target.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure 2 <span class="citation" data-cites="Singh2023">(<a href="#ref-Singh2023" role="doc-biblioref">Singh 2023</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="boxplot.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure 3</figcaption>
</figure>
</div>
</section>
<section id="statistical-modeling" class="level3">
<h3 class="anchored" data-anchor-id="statistical-modeling">Statistical Modeling</h3>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
</section>
</section>
<section id="references" class="level2 unnumbered">


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Abdulkareem2021" class="csl-entry" role="listitem">
Abdulkareem, Nasiba Mahdi, and Adnan Mohsin Abdulazeez. 2021. <span>“Machine Learning Classification Based on Radom Forest Algorithm: A Review.”</span> <em>International Journal of Science and Business</em> 5 (2): 128–42.
</div>
<div id="ref-Ali2012" class="csl-entry" role="listitem">
Ali, Jehad, Rehanullah Khan, Nasir Ahmad, and Imran Maqsood. 2012. <span>“Random Forests and Decision Trees.”</span> <em>International Journal of Computer Science Issues(IJCSI)</em> 9 (September).
</div>
<div id="ref-Aria2022" class="csl-entry" role="listitem">
Aria, Corrado &amp; Gnasso, Massimo &amp; Cuccurullo. 2022. <span>“A Comparison Among Interpretative Proposals for Random Forests.”</span> <em>R News</em> 10: 100438.
</div>
<div id="ref-Breiman2001" class="csl-entry" role="listitem">
Breiman, Leo. 2001. <span>“Random Forests.”</span> <em>Machine Learning</em> 45: 5–32. <a href="https://doi.org/10.1023/A:1010933404324">https://doi.org/10.1023/A:1010933404324</a>.
</div>
<div id="ref-Han2019" class="csl-entry" role="listitem">
Han, Sunwoo, and Hyunjoong Kim. 2019. <span>“On the Optimal Size of Candidate Feature Set in Random Forest.”</span> <em>Applied Sciences</em> 9 (5). <a href="https://doi.org/10.3390/app9050898">https://doi.org/10.3390/app9050898</a>.
</div>
<div id="ref-Jaiswal2017" class="csl-entry" role="listitem">
Jaiswal, Jitendra Kumar, and Rita Samikannu. 2017. <span>“Application of Random Forest Algorithm on Feature Subset Selection and Classification and Regression.”</span> In <em>2017 World Congress on Computing and Communication Technologies (WCCCT)</em>, 65–68. <a href="https://doi.org/10.1109/WCCCT.2016.25">https://doi.org/10.1109/WCCCT.2016.25</a>.
</div>
<div id="ref-Liaw2002" class="csl-entry" role="listitem">
Liaw, Andy, and Matthew Wiener. 2002. <span>“Classification and Regression by randomForest.”</span> <em>R News</em> 2 (3): 18–22.
</div>
<div id="ref-More2017" class="csl-entry" role="listitem">
More, A. S., and Dipti P. Rana. 2017. <span>“Review of Random Forest Classification Techniques to Resolve Data Imbalance.”</span> In <em>2017 1st International Conference on Intelligent Systems and Information Management (ICISIM)</em>, 72–78. <a href="https://doi.org/10.1109/ICISIM.2017.8122151">https://doi.org/10.1109/ICISIM.2017.8122151</a>.
</div>
<div id="ref-Schonlau2020" class="csl-entry" role="listitem">
Schonlau, Matthias, and Rosie Yuyan Zou. 2020. <span>“The Random Forest Algorithm for Statistical Learning.”</span> <em>The Stata Journal</em> 20: 3–29. <a href="https://doi.org/10.1177/1536867X20909688">https://doi.org/10.1177/1536867X20909688</a>.
</div>
<div id="ref-Singh2023" class="csl-entry" role="listitem">
Singh, Utkarsh. 2023. <span>“Heart Disease Prediction Dataset.”</span> <em>Kaggle</em>. <a href="https://www.kaggle.com/datasets/utkarshx27/heart-disease-diagnosis-dataset">https://www.kaggle.com/datasets/utkarshx27/heart-disease-diagnosis-dataset</a>.
</div>
<div id="ref-Tongtian2020" class="csl-entry" role="listitem">
Zhu, Tongtian. 2020. <span>“Analysis on the Applicability of the Random Forest.”</span> <em>Journal of Physics:Conference Series</em> 1607: 119–21.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>