---
title: "Random Forest Algorithm"
author: "Amy Brown, Jeff Eddy, Armani Harris"
date: "July 25, 2023"
format: revealjs
course: STA 6257 - Advance Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
---

## Introduction

## Literature Review

## Methods - Random Forest Algorithm {.smaller}

Random Forest algorithm is a supervised learning model consisting of multiple uncorrelated un-pruned decision trees creating a forest.

-   The algorithm can be used for both classification or regression models.
-   Two feature sampling methods reviewed: bootstrap and k-fold cross-validation.

## Random Forest Tree Structure {.smaller}

-   <b>Root Node:</b> Each tree starts with at a root node.
-   <b>Internal Nodes:</b> Each tree decision splits at the internal nodes using Gini Impurity or Permutation Importance for the feature importance.
-   <b>Leaf Node:</b> Each tree ends at the leaf node with the result stored as a categorical value for classification and numerical value regression.
-   <b>Final Result:</b> The final result is calculated by taking the majority vote for classification or the average for regression [@Abdulkareem2021].

![[@Abdulkareem2021]](RF%20flowchart.png)

## Tree Decision Splits {.smaller}

-   <b>Gini Impurity</b> is calculated by summing the Gini decrease across every tree in the forest and divided by the number of trees in the forest to provide an average.
-   <b>Permutation Importance</b> is calculated by evaluating the decrease in accuracy when a feature is randomly shuffled over all trees in the forest [@Jaiswal2017].

## Steps for Random Forest Algorithm {.smaller}

-   <b>Step 1:</b> Randomly select k features from a total of m features where k \< m.
-   <b>Step 2:</b> 3 features in step 1 using the best split point discussed in step 3.
-   <b>Step 3:</b> Each internal node is split into additional internal nodes using Gini Impurity or Permuation Importance.
-   <b>Step 4:</b> Repeat the previous steps 1 through 3 steps until 1 node is remaining.
-   <b>Step 5:</b> Continue to build the forest by repeating all previous steps for n number of times to create n number of trees [@Jaiswal2017].

## Dataset {.smaller}

-   Heart Disease Prediction dataset:\
    https://www.kaggle.com/datasets/utkarshx27/heart-disease-diagnosis-dataset
-   14 data fields (1 target and 13 features)
-   Target: Heart Disease - Predicting the presence or absence of heart disease
-   270 observations
-   No missing values [@Singh2023]

## Features and Target Fields {.smaller}

-   age
-   sex
-   chest pain type (4 values)
-   resting blood pressure
-   serum cholestoral in mg/dl
-   fasting blood sugar 120 mg/dl
-   resting electrocardiographic results (values 0,1,2)
-   maximum heart rate achieved
-   exercise induced angina
-   oldpeak = ST depression induced by exercise relative to rest
-   the slope of the peak exercise ST segment
-   number of major vessels (0-3) colored by flourosopy
-   thal: 3 = normal; 6 = fixed defect; 7 = reversable defect [@Singh2023]

```{r, message=FALSE}
library(caret)
library(boot)
library(tidymodels)
library(lindia)
library(car)
library(haven)
library(htmltools)
library(tidyverse)
library(dplyr)
library(fastDummies)
library(nnet)
library(randomForest)
library(gridExtra)
library(AppliedPredictiveModeling)
library("ggplot2")
library("reshape2")
```

```{r, message=FALSE}
data<- read.csv('C:/Users/amybr/OneDrive/Desktop/dataset_heart.csv')
```

## Visualization Boxplot #1 {.smaller}

An extreme outlier was observed at a value of 564 in the feature serum cholesterol. Mild outliers were identified in features resting blood pressure, serum cholesterol, and max heart rate.

```{r, message=FALSE}
# Split dataframe into dataframe for box plots only
data_gt_100=data
data_lt_100=data

# Boxplot age, serum cholesterol, resting blood pressure, and max heart rate
data_gt_100 = data %>% select(1,4,5,8)

# Reshape sample data to long form
data_gt_100_melt <- melt(data_gt_100)

# Add variable parameter for axis label in dataset
levels(data_gt_100_melt$variable) <- c("aging","resting.blood.pressure","serum.cholesterol","max.heart.rate")
  
# Draw boxplot
ggplot(data_gt_100_melt, aes(variable, value)) + 
geom_boxplot(width = .25)  +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```

## Visualization Boxplot #2 {.smaller}

Mild outliers were identified in features chest pain type, fasting blood sugar, old peak, and major vessels.

```{r}
# Boxplot of all other variables
data_lt_100 = data_lt_100 %>% select(2,3,6,7,9:14)

# Reshape sample data to long form
data_lt_100_melt <- melt(data_lt_100)

# Add variable parameter for axis label in dataset
levels(data_lt_100_melt$variable) <- c("sex","chest.pain.type","fasting.blood.sugar","resting.electrocardiographic.results", "exercise.induced.angina","oldpeak","ST.segment","major.vessels", "thal", "heart.disease")
  
# Draw boxplot
ggplot(data_lt_100_melt, aes(variable, value)) + 
geom_boxplot(width = .25)  +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```

## Removal of Extreme Outlier {.smaller}

Since the dataset is small, only the extreme outlier was removed.  Boxplot #1 was rerun to verify the extreme outlier of 564 was removed from feature serum cholesterol.

```{r, message=FALSE}
# Remove extreme outlier
data_gt_100 = data_gt_100[data$serum.cholestoral != "564", ]
data1=data
data1 = data1[data1$serum.cholestoral != "564", ]

# Rerun boxplot after extreme outlier removed 

# Reshape sample data to long form
data_gt_100_melt <- melt(data_gt_100)

# Add variable parameter for axis label in dataset
levels(data_gt_100_melt$variable) <- c("aging","resting.blood.pressure","serum.cholesterol","max.heart.rate")
  
# Draw boxplot
ggplot(data_gt_100_melt, aes(variable, value)) + 
geom_boxplot(width = .25)  +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```

## Data Transformation {.smaller}

-   Features Heart Disease and Thal were set to a factor variable

-   Feature Thal was transformed as follows:

    -   Value 3 to normal

    -   Value 6 to fixed defect

    -   Value 7 to reversible defect

```{r, message=FALSE}
data$heart.disease<- as.factor(data$heart.disease)
data1$thal[data1$thal== 3]<-"normal"
data1$thal[data1$thal== 6]<-"fixed defect"
data1$thal[data1$thal== 7]<-"reversible defect"
data1$thal<- as.factor(data1$thal)
```

## Model Training

## Predictive Modeling

## Conclusion

## References
