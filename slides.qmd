---
title: "Random Forest"
author: "Armani Harris, Amy Brown, Jeff Eddy"
date: "July 31, 2023"
format: revealjs
course: STA 6257 - Advance Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
---

## Introduction {style="font-size: 80%;"}

Random Forests (RF) is a popular ensemble learning algorithm that combines multiple decision trees to improve predictive accuracy and handle complex datasets. This literature review aims to explore ten articles that discuss various aspects of the Random Forest algorithm, including its theoretical foundation, feature selection, data imbalance resolution, interpretability, and comparisons with other classification methods. The selected articles offer valuable insights into the strengths, weaknesses, and practical applications of Random Forest in different domains.

## Literature Review {style="font-size: 80%;"}

Breiman [@Breiman2001]

-   Introduced the concept of Random Forests and highlighted the importance of the Law of Large Numbers in avoiding overfitting.

Liaw and Wiener [@Liaw2002]

-   Expanded on this work, focusing on the classification and regression capabilities of RF.

Jaiswal, K. & Samikannu, R. [@Jaiswal2017]

-   Discussed the use of bagging and highlighted the significance of variable importance in RF models.When dealing with datasets containing a high number of variables the importance of feature selection.

## Literature Review cont'd {style="font-size: 80%;"}

Han and Kim [@Han2019] - Investigated the optimal size of candidate feature sets in Random Forest for classification and regression tasks.

-   The study reveals that the default candidate feature set is not always the best performing and that the optimal size can vary depending on the dataset used . The potential challenges such as overfitting and handling multi-valued attributes exist when using the Random Forest (RF) algorithm.

Abdulkareem, N. M., & Abdulazeez, A.[@Abdulkareem2021]

-   The advantages emphasized in this article entails accuracy, efficiency with large datasets, and minimal data preprocessing requirements.

## Methods - Random Forest Algorithm {style="font-size: 80%;"}

-   The algorithm consists of multiple uncorrelated un-pruned decision trees creating a forest.
-   The algorithm is a supervised learning model.
-   The algorithm can be used for both classification or regression models.
-   Two sampling methods reviewed: k-fold cross-validation and bootstrap.

## Random Forest Decision Tree Structure {style="font-size: 70%;"}

-   **Root Node:** Each tree starts with at a root node.
-   **Internal Nodes:** Each tree decision splits at the internal nodes using Gini Impurity or Permutation Importance for the feature importance.
-   **Leaf Node:** Each tree ends at the leaf node with the result stored as a categorical value for classification or numerical value for regression.
-   **Final Result:** The final result is calculated by taking the majority vote for classification or the average for regression [@Abdulkareem2021].

![Figure 1 [@Abdulkareem2021]](RF_flowchart.png)

## Tree Decision Splits {style="font-size: 80%;"}

-   **Gini Impurity** is calculated by summing the Gini decrease across every tree in the forest and divided by the number of trees in the forest to provide an average.
-   <b>Permutation Importance</b> is calculated by evaluating the decrease in accuracy when a feature is randomly shuffled over all trees in the forest [@Jaiswal2017].

## Steps for Random Forest Algorithm {style="font-size: 80%;"}

-   **Step 1:** Randomly select k features from a total of m features where k \< m.
-   **Step 2:** Specify a node size d for the features selected in step 1 using the best split point discussed in step 3.
-   **Step 3:** Calculate the best split point at the internal node using Gini Impurity or Permuation Importance.
-   **Step 4:** Repeat the previous steps 1 through 3 until 1 node is remaining.
-   **Step 5:** Continue to build the forest by repeating all previous steps for n number of times to create n number of trees [@Jaiswal2017].

## Dataset {style="font-size: 80%;"}

-   Heart Disease Prediction dataset:\
    https://www.kaggle.com/datasets/utkarshx27/heart-disease-diagnosis-dataset
-   14 data fields (1 target and 13 features)
-   Target: Heart Disease - Predicting the presence or absence of heart disease
-   270 observations
-   No missing values [@Singh2023]

## Features and Target Fields  {style="font-size: 65%;"}

-   age
-   sex
-   chest pain type (4 values)
-   resting blood pressure
-   serum cholestoral in mg/dl
-   fasting blood sugar 120 mg/dl
-   resting electrocardiographic results (values 0, 1, 2)
-   maximum heart rate achieved
-   exercise induced angina
-   oldpeak = ST depression induced by exercise relative to rest
-   the slope of the peak exercise ST segment
-   number of major vessels (0-3) colored by flourosopy
-   thal: 3 = normal; 6 = fixed defect; 7 = reversable defect 
-   heart disease (Target: (absence (1) or presence (2)) \
    [@Singh2023]

## R Packages {style="font-size: 70%;"}

-   **caret package**

    -   Versatile package used for various classification and regression models.

    -   Allows tuning of the model through parameters set in the package.

        -   Parameter method set to rf for Random Forest algorithm.

        -   Parameter trcontrol sets the sampling method for the dataset.

        -   Parameter metric set to measure feature importance such as Accuracy and Gini Impurity [@Johnson2023].

-   **dplyr package**

    -   Required for filtering dataframes and to produce the box plots of the data fields.

-   **reshape2 package**

    -   Required to reshape the data to the correct format to build the box plots.

```{r, message=FALSE}
library(caret)
library(dplyr)
library("reshape2")
```

```{r, message=FALSE}
data<- read.csv('C:/Users/amybr/OneDrive/Desktop/dataset_heart.csv')
```

## Visualization Box Plot #1 {style="font-size: 80%;"}

An extreme outlier was observed at a value of 564 in the feature serum cholesterol. Mild outliers were identified in features resting blood pressure, serum cholesterol, and max heart rate.

```{r, message=FALSE}
# Split dataframe into dataframe for box plots only
data_gt_100=data
data_lt_100=data

# Box plot age, serum cholesterol, resting blood pressure, and max heart rate
data_gt_100 = data %>% select(1,4,5,8)

# Reshape sample data to long form
data_gt_100_melt <- melt(data_gt_100)

# Add variable parameter for axis label in dataset
levels(data_gt_100_melt$variable) <- c("aging","resting.blood.pressure","serum.cholesterol","max.heart.rate")
  
# Draw box plot
ggplot(data_gt_100_melt, aes(variable, value)) + 
geom_boxplot(width = .25)  +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```

## Visualization Box Plot #2 {style="font-size: 80%;"}

Mild outliers were identified in features chest pain type, fasting blood sugar, old peak, and major vessels.

```{r}
# Box plot of all other variables
data_lt_100 = data_lt_100 %>% select(2,3,6,7,9:14)

# Reshape sample data to long form
data_lt_100_melt <- melt(data_lt_100)

# Add variable parameter for axis label in dataset
levels(data_lt_100_melt$variable) <- c("sex","chest.pain.type","fasting.blood.sugar","resting.electrocardiographic.results", "exercise.induced.angina","oldpeak","ST.segment","major.vessels", "thal", "heart.disease")
  
# Draw box plot
ggplot(data_lt_100_melt, aes(variable, value)) + 
geom_boxplot(width = .25)  +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```

## Removal of Extreme Outlier {style="font-size: 80%;"}

Since the dataset is small, only the extreme outlier was removed. Box plot #1 was rerun to verify the extreme outlier of 564 was removed from feature serum cholesterol.

```{r, message=FALSE}
# Remove extreme outlier
data_gt_100 = data_gt_100[data$serum.cholestoral != "564", ]
data1=data
data1 = data1[data1$serum.cholestoral != "564", ]

# Rerun box plot after extreme outlier removed 

# Reshape sample data to long form
data_gt_100_melt <- melt(data_gt_100)

# Add variable parameter for axis label in dataset
levels(data_gt_100_melt$variable) <- c("aging","resting.blood.pressure","serum.cholesterol","max.heart.rate")
  
# Draw box plot
ggplot(data_gt_100_melt, aes(variable, value)) + 
geom_boxplot(width = .25)  +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```

## Data Transformation {style="font-size: 80%;"}

-   Random forest classification model requires a dependent factor variable.
-   Target variable heart disease was transformed to a factor variable.

```{r, message=FALSE}
data1$heart.disease<- as.factor(data1$heart.disease)
```

## Model Training {style="font-size: 80%;"}

The target variable was binary with categories of no heart disease (1) and heart disease (2).

-   269 observations

-   Data set was split for a training model and a testing model at a 0.7 ratio.

-   189 observations in the training data set and 80 in the testing data set.

```{r, message=FALSE}
set.seed(123)
split <- createDataPartition(data1$heart.disease, p = 0.7, list = FALSE)
train <- data1[split, ]
test <- data1[-split, ]
```

## K-Fold Cross-Validation: Default Training Model {style="font-size: 80%;"}

-   13 Predictor Features

-   500 Decision Trees, 2 Features considered

-   Accuracy = 0.8358

```{r, message=FALSE}
set.seed(123)
# Define the control
trControl <- trainControl(method = "cv",
    number = 10,
    search = "grid")
#model with default values
rf_default <- train(heart.disease~.,
    data = train,
    method = "rf",
    metric = "Accuracy",
    trControl = trControl)
# Print the results
print(rf_default)
```

## Bootstrap: Default Training Model {style="font-size: 80%;"}

-   13 Predictor Features

-   500 Decision Trees, 2 Features considered

-   Accuracy = .8322

```{r, message=FALSE}
set.seed(123)
fit_rf2 <- train(heart.disease~.,
    data = train,
    method = "rf",
    importance = TRUE)
print(fit_rf2)
```

## Bootstrap Versus K-fold Cross-Validation {style="font-size: 80%;"}

-   Bootstrap randomly selects n number of observations with replacement.

-   K-Fold Cross-validation creates subsets of the data then randomly shuffles the observations into the subset "folds".

-   K-Fold trial was more accurate 0.8358 vs 0.8322.

## K-Fold Cross-Validation: Tuning the Model {style="font-size: 60%;"}

**Selecting the best possible number of features first**
```{r, message=FALSE}
#Search best mtry
set.seed(123)
tuneGrid <- expand.grid(.mtry = c(1: 10))
rf_mtry <- train(heart.disease~.,
    data = train,
    method = "rf",
    metric = "Accuracy",
    tuneGrid = tuneGrid,
    trControl = trControl,
    importance = TRUE,
    nodesize = 14,
    ntree = 300)
print(rf_mtry)

best_mtry <- rf_mtry$bestTune$mtry 
```

## K-Fold Cross-Validation: Optimal Number of Nodes {style="font-size: 70%;"}

```{r, message=FALSE}
# Search maxnodes - look for highest accuracy
store_maxnode <- list()
# Assigns best mtry to tuneGrid
tuneGrid <- expand.grid(.mtry = best_mtry)
for (maxnodes in c(5: 15)) {
    set.seed(123)
    rf_maxnode <- train(heart.disease~.,
        data = train,
        method = "rf",
        metric = "Accuracy",
        tuneGrid = tuneGrid,
        trControl = trControl,
        importance = TRUE,
        nodesize = 14,
        maxnodes = maxnodes,
        ntree = 300)
    current_iteration <- toString(maxnodes)
    store_maxnode[[current_iteration]] <- rf_maxnode
}
results_mtry <- resamples(store_maxnode)
summary(results_mtry, metric="Accuracy")
```

## K-Fold Cross-Validation: Ideal Number of Trees {style="font-size: 70%;"}

```{r, message=FALSE}
# Search best ntree
store_maxtrees <- list()
for (ntree in c(100, 200, 300, 400, 500, 600, 700, 800, 900, 1000)) {
    set.seed(123)
    rf_maxtrees <- train(heart.disease~.,
        data = train,
        method = "rf",
        metric = "Accuracy",
        tuneGrid = tuneGrid,
        trControl = trControl,
        importance = TRUE,
        nodesize = 14,
        maxnodes = 6,
        ntree = ntree)
    key <- toString(ntree)
    store_maxtrees[[key]] <- rf_maxtrees
}
results_tree <- resamples(store_maxtrees)
summary(results_tree, metric="Accuracy")
```

## K-Fold Cross-Validation: Training the Final Model {style="font-size: 65%;"}

-   13 Predictor Features

-   500 Decision Trees, 2 Features considered

-   Default Training Model Accuracy = 0.8358

Tuned Model uses:

-   300 Trees

-   6 nodes

-   2 Features

-   Accuracy = .8674

```{r, message=FALSE}
# Train final model
set.seed(123)
fit_rf <- train(heart.disease~.,
    data = train,
    method = "rf",
    metric = "Accuracy",
    tuneGrid = tuneGrid,
    trControl = trControl,
    importance = TRUE,
    nodesize = 14,
    ntree = 300,
    maxnodes = 6)
print(fit_rf)
```

## Bootstrap: Training the Final Model {style="font-size: 70%;"}

```{r, message=FALSE}
# Train final model
set.seed(123)
fit_rf2 <- train(heart.disease~.,
    data = train,
    method = "rf",
    tuneGrid = tuneGrid,
    importance = TRUE,
    nodesize = 14,
    ntree = 300,
    maxnodes = 6)
print(fit_rf2)
```

## Final Model Comparison {style="font-size: 80%;"}

-   K-Fold Cross-Validation Accuracy = .8674

-   Bootstrap Accuracy = .8345

-   Predictive Modeling will use K-Fold Cross-Validation due to higher accuracy

## Predictive Modeling: Most Influential Features {style="font-size: 55%;"}

-   Major Vessels

-   Thal

-   Chest Pain Type

-   Old Peak

-   Maximum Heart Rate
```{r, message=FALSE}
# Evaluate model
prediction <-predict(fit_rf, test)

#varImp(fit_rf)
rfImp<- varImp(fit_rf)
ggplot(rfImp, aes(x = importance(), y = value)) +
  geom_bar(stat = "identity", linewidth = 0.5,fill="blue")
```

## Predictive Model Accuracy {style="font-size: 70%;"}

-   83.8% (0.8375, CI: 0.7382, 0.9105, p\< 0.001)

-   Sensitivity and specificity were calculated at 0.909 and 0.75 respectively

-   Estimated false positive rate of 30.77%

```{r, message=FALSE}
cm <- confusionMatrix(prediction, test$heart.disease)

draw_confusion_matrix <- function(cm) {
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # Create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'Positive', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Negative', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'True', cex=1.2, srt=90)
  text(140, 335, 'False', cex=1.2, srt=90)

  # Add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # Add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # Add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  

draw_confusion_matrix(cm)
```

## Conclusion {style="font-size: 70%;"}

The Random Forest algorithm includes two feature sampling methods:

-  K-fold cross-validation

-  Bootstrap

The algorithm can be applied to both classification and regression tasks, with the prediction determined by the majority vote or average of the leaf nodes, respectively.

The algorithm is more cost effective and more reliable in understanding of the data.

## Questions?

## References {style="font-size: 60%;"}
